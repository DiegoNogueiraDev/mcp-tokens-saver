#!/usr/bin/env node
import OpenAI from 'openai';
import 'dotenv/config';

const openai = new OpenAI({
  apiKey: process.env.MOONSHOT_API_KEY,
  baseURL: 'https://api.moonshot.ai/v1'
});

async function testMoonshotConnection() {
  console.log('ğŸ§ª Testando conexÃ£o com Moonshot AI...\n');

  try {
    // Teste 1: ConexÃ£o bÃ¡sica
    console.log('1ï¸âƒ£ Teste de conexÃ£o bÃ¡sica:');
    const basicResponse = await openai.chat.completions.create({
      model: 'moonshot-v1-8k',
      messages: [
        { role: 'user', content: 'Responda apenas "OK" se vocÃª estÃ¡ funcionando.' }
      ]
    });

    console.log('âœ… Resposta:', basicResponse.choices[0].message.content);
    console.log('ğŸ“Š Tokens usados:', basicResponse.usage?.total_tokens);
    console.log();

    // Teste 2: Context Caching com headers alternativos
    console.log('2ï¸âƒ£ Teste de Context Caching com headers:');
    
    const systemPrompt = 'VocÃª Ã© um assistente especializado em economia de tokens. Sempre responda de forma concisa e direta.';
    
    // Primeira requisiÃ§Ã£o - tentativa de cache via headers
    console.log('ğŸ“¦ Testando cache via headers X-Msh-Context-Cache...');
    const headerResponse = await openai.chat.completions.create({
      model: 'moonshot-v1-8k',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: 'Explique em uma frase o que Ã© cache de contexto.' }
      ]
    }, {
      headers: {
        'X-Msh-Context-Cache': 'true',
        'X-Msh-Context-Cache-Reset-TTL': '3600'
      }
    });

    console.log('âœ… Resposta com headers:', headerResponse.choices[0].message.content);
    console.log('ğŸ“Š Tokens usados:', headerResponse.usage?.total_tokens);
    console.log('ğŸ¯ Tokens em cache:', (headerResponse.usage as any)?.cached_tokens || 'N/A');
    console.log();

    // Teste 3: Segunda requisiÃ§Ã£o com mesmo contexto
    console.log('3ï¸âƒ£ Teste de reutilizaÃ§Ã£o de contexto:');
    const secondResponse = await openai.chat.completions.create({
      model: 'moonshot-v1-8k',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: 'Como economizar tokens em APIs de LLM?' }
      ]
    }, {
      headers: {
        'X-Msh-Context-Cache': 'true',
        'X-Msh-Context-Cache-Reset-TTL': '3600'
      }
    });

    console.log('âœ… Segunda resposta:', secondResponse.choices[0].message.content?.substring(0, 100) + '...');
    console.log('ğŸ“Š Tokens totais:', secondResponse.usage?.total_tokens);
    console.log('ğŸ¯ Tokens em cache:', (secondResponse.usage as any)?.cached_tokens || 'N/A');
    console.log();

    // Teste 4: EstatÃ­sticas de economia
    console.log('4ï¸âƒ£ EstatÃ­sticas de economia:');
    const baseTokens = systemPrompt.length / 4;
    const firstSavedTokens = (headerResponse.usage as any)?.cached_tokens || 0;
    const secondSavedTokens = (secondResponse.usage as any)?.cached_tokens || 0;
    const firstEconomyPercent = firstSavedTokens > 0 ? ((firstSavedTokens / headerResponse.usage!.total_tokens!) * 100).toFixed(1) : '0';
    const secondEconomyPercent = secondSavedTokens > 0 ? ((secondSavedTokens / secondResponse.usage!.total_tokens!) * 100).toFixed(1) : '0';
    
    console.log(`ğŸ’° Tokens base do sistema: ~${Math.ceil(baseTokens)}`);
    console.log(`ğŸ¯ Primeira req - Tokens salvos: ${firstSavedTokens} (${firstEconomyPercent}%)`);
    console.log(`ğŸ¯ Segunda req - Tokens salvos: ${secondSavedTokens} (${secondEconomyPercent}%)`);
    console.log();

    console.log('ğŸ‰ Todos os testes passaram! MCP Token Saver estÃ¡ pronto para uso.');

  } catch (error) {
    console.error('âŒ Erro nos testes:', error);
    console.error('\nğŸ”§ Verificar:');
    console.error('- MOONSHOT_API_KEY estÃ¡ configurada no .env');
    console.error('- API key tem permissÃµes para Context Caching');
    console.error('- ConexÃ£o com api.moonshot.ai estÃ¡ funcionando');
  }
}

// FunÃ§Ã£o para testar funcionalidades do MCP
async function testMCPFunctionality() {
  console.log('\nğŸ› ï¸ Testando funcionalidades MCP especÃ­ficas...\n');

  const testPrompts = [
    'Analyze this code: function hello() { return "world"; }',
    'Review this React component for improvements',
    'Explain how async/await works in JavaScript',
    'Generate documentation for a REST API'
  ];

  for (let i = 0; i < testPrompts.length; i++) {
    const prompt = testPrompts[i];
    console.log(`ğŸ“ Teste ${i + 1}: ${prompt.substring(0, 40)}...`);
    
    try {
      const response = await openai.chat.completions.create({
        model: 'moonshot-v1-8k',
        messages: [
          { role: 'system', content: 'VocÃª Ã© um assistente de programaÃ§Ã£o conciso.' },
          { role: 'user', content: prompt }
        ]
      });

      const tokens = response.usage?.total_tokens || 0;
      console.log(`   âœ… Tokens: ${tokens} | Cache elegÃ­vel: ${shouldCacheHeuristic(prompt, tokens)}`);
      
    } catch (error) {
      console.log(`   âŒ Erro: ${error}`);
    }
  }
}

// HeurÃ­stica de cache (mesma do servidor)
function shouldCacheHeuristic(prompt: string, tokens: number): boolean {
  return tokens > 100 && (
    prompt.includes('analyze') ||
    prompt.includes('review') ||
    prompt.includes('explain') ||
    prompt.includes('generate') ||
    prompt.length > 500
  );
}

async function main() {
  await testMoonshotConnection();
  await testMCPFunctionality();
}

main().catch(console.error);